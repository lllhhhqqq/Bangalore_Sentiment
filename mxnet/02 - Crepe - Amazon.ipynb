{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSUMMARY:\\nAmazon pos/neg sentiment classification\\n\\nAccuracy: 0.94\\nTime per Epoch: 9550 seconds = 220 rev/s\\nTotal time: 9550*10 = 1592 min = 26.5 hours\\nTrain size = 2,097,152\\nTest size = 233,016\\n\\nDETAILS:\\nAttempt to replicate crepe model using MXNET:\\nhttps://github.com/zhangxiangxiao/Crepe\\n\\nThis uses an efficient numpy array (dtype=bool)\\nto hold all data in RAM. \\n\\nRun on one GPU (Tesla K80) with batch=128\\nPeak RAM: 142GB, and training cut to: 2,097,152 (from 3.6M)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "SUMMARY:\n",
    "Amazon pos/neg sentiment classification\n",
    "\n",
    "Accuracy: 0.94\n",
    "Time per Epoch: 9550 seconds = 220 rev/s\n",
    "Total time: 9550*10 = 1592 min = 26.5 hours\n",
    "Train size = 2,097,152\n",
    "Test size = 233,016\n",
    "\n",
    "DETAILS:\n",
    "Attempt to replicate crepe model using MXNET:\n",
    "https://github.com/zhangxiangxiao/Crepe\n",
    "\n",
    "This uses an efficient numpy array (dtype=bool)\n",
    "to hold all data in RAM. \n",
    "\n",
    "Run on one GPU (Tesla K80) with batch=128\n",
    "Peak RAM: 142GB, and training cut to: 2,097,152 (from 3.6M)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import mxnet as mx\n",
    "import wget\n",
    "import time\n",
    "import os.path\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'amazon': {\n",
    "        'FEATURE_LEN': 1014,\n",
    "        'LOG_FILE': 'crepe_amazon.log',\n",
    "        'AZ_ACC': 'amazonsentimenik',\n",
    "        'AZ_CONTAINER': 'textclassificationdatasets',\n",
    "        'TRAIN_DATA_FILE': 'amazon_review_polarity_train.csv',\n",
    "        'TEST_DATA_FILE': 'amazon_review_polarity_test.csv',\n",
    "        'SAVE_RESULT_FILE': 'crepe_predict_sentiment_amazon.csv'\n",
    "        \n",
    "    },\n",
    "    'twitter': {\n",
    "        'FEATURE_LEN': 140,\n",
    "        'LOG_FILE': 'crepe_twitter.log',\n",
    "        'AZ_ACC': 'thomasdelteillondon',\n",
    "        'AZ_CONTAINER': 'public',\n",
    "        'TRAIN_DATA_FILE': 'en-pre.train.txt',\n",
    "        'TEST_DATA_FILE': 'en-pre.test.txt',\n",
    "        'SAVE_RESULT_FILE': 'crepe_predict_sentiment_twitter.csv'\n",
    "    }\n",
    "}\n",
    "experiment = params['twitter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "AZ_ACC = experiment['AZ_ACC']\n",
    "AZ_CONTAINER = experiment['AZ_CONTAINER']\n",
    "\n",
    "ALPHABET = list(\"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+ =<>()[]{}\")\n",
    "FEATURE_LEN = experiment['FEATURE_LEN']\n",
    "BATCH_SIZE = 128\n",
    "NUM_FILTERS = 256\n",
    "DATA_SHAPE = (BATCH_SIZE, 1, FEATURE_LEN, len(ALPHABET))\n",
    "\n",
    "ctx = mx.gpu()\n",
    "EPOCHS = 10\n",
    "SD = 0.05  # std for gaussian distribution\n",
    "NOUTPUT = 2  # good or bad\n",
    "INITY = mx.init.Normal(sigma=SD)\n",
    "LR = 0.01\n",
    "MOMENTUM = 0.9\n",
    "WDECAY = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# logging\n",
    "logger = logging.getLogger()\n",
    "fhandler = logging.FileHandler(filename=experiment['LOG_FILE'], mode='a')\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fhandler.setFormatter(formatter)\n",
    "logger.addHandler(fhandler)\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_file(url):\n",
    "    # Create file-name\n",
    "    local_filename = url.split('/')[-1]\n",
    "    if os.path.isfile(local_filename):\n",
    "        pass\n",
    "        # print(\"The file %s already exist in the current directory\\n\" % local_filename)\n",
    "    else:\n",
    "        # Download\n",
    "        print(\"downloading ...\\n\")\n",
    "        wget.download(url)\n",
    "        print('\\nsaved data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data_frame(infile, shuffle = False):\n",
    "    print(\"processing data frame: %s\" % infile)\n",
    "    # Get data from windows blob\n",
    "    download_file('https://%s.blob.core.windows.net/%s/%s' % (AZ_ACC, AZ_CONTAINER, infile))\n",
    "    \n",
    "    # 3.6 mill is too much, use 2 mill (keep same ratio)\n",
    "    if \"test\" in infile:\n",
    "        maxrows = int(2097152/9)  # 16,384 batches\n",
    "    elif \"train\" in infile:\n",
    "        maxrows = int(2097152)\n",
    "\n",
    "    # load data into dataframe\n",
    "    df = pd.read_csv(infile,\n",
    "                     header=None,\n",
    "                     names=['sentiment', 'summary', 'text'],\n",
    "                     nrows=maxrows)\n",
    "    # Shuffle\n",
    "    if shuffle:\n",
    "        df = df.sample(frac=1).reset_index(drop=True)\n",
    "        \n",
    "    # concat summary, review; trim to 1014 char; reverse; lower\n",
    "    df['rev'] = df.apply(lambda x: \"%s %s\" % (x['summary'], x['text']), axis=1)\n",
    "    df.rev = df.rev.str[:FEATURE_LEN].str[::-1].str.lower()\n",
    "    \n",
    "    # store class as nparray\n",
    "    #df.sentiment -= 1\n",
    "    y_split = np.asarray(df.sentiment, dtype='bool')\n",
    "    # drop columns\n",
    "    df.drop(['text', 'summary', 'sentiment'], axis=1, inplace=True)\n",
    "\n",
    "    # Dictionary to create character vectors\n",
    "    character_hash = pd.DataFrame(np.identity(len(ALPHABET), dtype='bool'), columns=ALPHABET)\n",
    "    print(\"finished processing data frame: %s\" % infile)\n",
    "    print(\"data contains %d obs\" % df.shape[0])\n",
    "    batch_size = df.shape[0]\n",
    "    # Create encoding\n",
    "    X_split = np.zeros([batch_size, 1, FEATURE_LEN, len(ALPHABET)], dtype='bool')\n",
    "    # Main loop\n",
    "    for ti, tx in enumerate(df.rev):\n",
    "        if (ti+1) % (100*1000) == 0:\n",
    "            print(\"Processed: \", ti+1)\n",
    "        chars = list(tx)\n",
    "        for ci, ch in enumerate(chars):\n",
    "            if ch in ALPHABET:\n",
    "                X_split[ti % batch_size][0][ci] = np.array(character_hash[ch], dtype='bool')\n",
    "                \n",
    "    # Return as a DataBatch\n",
    "    #return DataBatch(data=[mx.nd.array(X_split)],\n",
    "    #                 label=[mx.nd.array(y_split[ti + 1 - batch_size:ti + 1])])\n",
    "    return X_split, y_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_crepe():\n",
    "    \"\"\"\n",
    "    Replicating: https://github.com/zhangxiangxiao/Crepe/blob/master/train/config.lua\n",
    "    \"\"\"\n",
    "    input_x = mx.sym.Variable('data')  # placeholder for input\n",
    "    input_y = mx.sym.Variable('softmax_label')  # placeholder for output\n",
    "    # 1. alphabet x 1014\n",
    "    conv1 = mx.symbol.Convolution(\n",
    "        data=input_x, kernel=(7, 69), num_filter=NUM_FILTERS)\n",
    "    relu1 = mx.symbol.Activation(\n",
    "        data=conv1, act_type=\"relu\")\n",
    "    pool1 = mx.symbol.Pooling(\n",
    "        data=relu1, pool_type=\"max\", kernel=(3, 1), stride=(3, 1))\n",
    "    # 2. 336 x 256\n",
    "    conv2 = mx.symbol.Convolution(\n",
    "        data=pool1, kernel=(7, 1), num_filter=NUM_FILTERS)\n",
    "    relu2 = mx.symbol.Activation(\n",
    "        data=conv2, act_type=\"relu\")\n",
    "    pool2 = mx.symbol.Pooling(\n",
    "        data=relu2, pool_type=\"max\", kernel=(3, 1), stride=(3, 1))\n",
    "    # 3. 110 x 256\n",
    "    conv3 = mx.symbol.Convolution(\n",
    "        data=pool2, kernel=(3, 1), num_filter=NUM_FILTERS)\n",
    "    relu3 = mx.symbol.Activation(\n",
    "        data=conv3, act_type=\"relu\")\n",
    "    # 4. 108 x 256\n",
    "    conv4 = mx.symbol.Convolution(\n",
    "        data=relu3, kernel=(3, 1), num_filter=NUM_FILTERS)\n",
    "    relu4 = mx.symbol.Activation(\n",
    "        data=conv4, act_type=\"relu\")\n",
    "    # 5. 106 x 256\n",
    "    conv5 = mx.symbol.Convolution(\n",
    "        data=relu4, kernel=(3, 1), num_filter=NUM_FILTERS)\n",
    "    relu5 = mx.symbol.Activation(\n",
    "        data=conv5, act_type=\"relu\")\n",
    "    # 6. 104 x 256\n",
    "    conv6 = mx.symbol.Convolution(\n",
    "        data=relu5, kernel=(3, 1), num_filter=NUM_FILTERS)\n",
    "    relu6 = mx.symbol.Activation(\n",
    "        data=conv6, act_type=\"relu\")\n",
    "    pool6 = mx.symbol.Pooling(\n",
    "        data=relu6, pool_type=\"max\", kernel=(3, 1), stride=(3, 1))\n",
    "    # 34 x 256\n",
    "    flatten = mx.symbol.Flatten(data=pool6)\n",
    "    # 7.  8704\n",
    "    fc1 = mx.symbol.FullyConnected(\n",
    "        data=flatten, num_hidden=1024)\n",
    "    act_fc1 = mx.symbol.Activation(\n",
    "        data=fc1, act_type=\"relu\")\n",
    "    drop1 = mx.sym.Dropout(act_fc1, p=0.5)\n",
    "    # 8. 1024\n",
    "    fc2 = mx.symbol.FullyConnected(\n",
    "        data=drop1, num_hidden=1024)\n",
    "    act_fc2 = mx.symbol.Activation(\n",
    "        data=fc2, act_type=\"relu\")\n",
    "    drop2 = mx.sym.Dropout(act_fc2, p=0.5)\n",
    "    # 9. 1024\n",
    "    fc3 = mx.symbol.FullyConnected(\n",
    "        data=drop2, num_hidden=NOUTPUT)\n",
    "    crepe = mx.symbol.SoftmaxOutput(\n",
    "        data=fc3, label=input_y, name=\"softmax\")\n",
    "    return crepe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualise symbol (for crepe)\n",
    "crepe = create_crepe()\n",
    "\n",
    "#a = mx.viz.plot_network(crepe)\n",
    "#a.render('Crepe Model')\n",
    "#a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data frame: en-pre.train.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thdeltei\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2821: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished processing data frame: en-pre.train.txt\n",
      "data contains 489056 obs\n",
      "Processed:  100000\n",
      "Processed:  200000\n",
      "Processed:  300000\n",
      "Processed:  400000\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y = load_data_frame(experiment['TRAIN_DATA_FILE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(489056, 1, 140, 69)\n",
      "(489056,)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iter = mx.io.NDArrayIter(train_x, train_y, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del train_x\n",
    "del train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = mx.model.FeedForward(\n",
    "    ctx = ctx,\n",
    "    symbol = create_crepe(), \n",
    "    num_epoch = EPOCHS,  # number of training rounds\n",
    "    learning_rate = LR,  # learning rate\n",
    "    momentum = MOMENTUM,   # momentum for sgd\n",
    "    wd = WDECAY,  # weight decay for reg\n",
    "    initializer = INITY  # init with sd of 0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "model.fit(\n",
    "    X = train_iter,\n",
    "    eval_metric='accuracy',\n",
    "    batch_end_callback=mx.callback.Speedometer(BATCH_SIZE),\n",
    "    epoch_end_callback=mx.callback.do_checkpoint(\"crepe_checkp_\") \n",
    ")\n",
    "\n",
    "print(\"Finished training in %.0f seconds\" % (time.time() - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del train_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# GPU broke after completing 7th epoch\n",
    "# Re-load checkpoint\n",
    "# If training breaks (happens on GPU), we can train further like so:\n",
    "\n",
    "# Load trained model:\n",
    "pretrained_model = mx.model.FeedForward.load(\"crepe_checkp_v2_\", 7)  \n",
    "\n",
    "model = mx.model.FeedForward(\n",
    "    ctx = ctx,\n",
    "    symbol=pretrained_model.symbol,\n",
    "    arg_params=pretrained_model.arg_params,\n",
    "    aux_params=pretrained_model.aux_params,\n",
    "    num_epoch=11, begin_epoch=7\n",
    ")\n",
    "\n",
    "# Train remaining epochs\n",
    "tic = time.time()\n",
    "model.fit(\n",
    "    X = train_iter,\n",
    "    eval_metric=['accuracy'],\n",
    "    batch_end_callback=mx.callback.Speedometer(100*BATCH_SIZE),\n",
    "    epoch_end_callback=mx.callback.do_checkpoint(\"crepe_checkp_v2_\") \n",
    ")\n",
    "\n",
    "print(\"Finished training in %.0f seconds\" % (time.time() - tic))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_x, test_y = load_data_frame(experiment['TEST_DATA_FILE'])\n",
    "test_iter = mx.io.NDArrayIter(test_x, test_y, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "pred = np.argsort(model.predict(X = test_iter))[:,-1]\n",
    "\n",
    "# Save Results\n",
    "np.savetxt(experiment['SAVE_RESULT_FILE'], np.c_[pred, test_y], delimiter=',', fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94166495004634876"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy\n",
    "acc = sum(pred==test_y.astype('int'))/float(len(test_y))\n",
    "logger.info(acc)\n",
    "acc  # 0.94"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Extract\n",
    "\n",
    "\n",
    "```\n",
    "2016-08-24 21:11:28,407 - root - INFO - Start training with [gpu(0)]\n",
    "2016-08-24 21:12:40,015 - root - INFO - Epoch[0] Batch [50]\t Speed: 114.7797 samples/sec\tTrain-accuracy=0.502031\n",
    "2016-08-24 21:13:15,326 - root - INFO - Epoch[0] Batch [100]\tSpeed: 181.3185 samples/sec\tTrain-accuracy=0.514531\n",
    "2016-08-24 21:13:46,977 - root - INFO - Epoch[0] Batch [150]\tSpeed: 202.2053 samples/sec\tTrain-accuracy=0.515938\n",
    "2016-08-24 21:14:17,447 - root - INFO - Epoch[0] Batch [200]\tSpeed: 210.0496 samples/sec\tTrain-accuracy=0.522344\n",
    "2016-08-24 21:14:48,170 - root - INFO - Epoch[0] Batch [250]\tSpeed: 208.3130 samples/sec\tTrain-accuracy=0.540156\n",
    "...\n",
    "2016-08-26 20:53:13,394 - root - INFO - Epoch[10] Batch [16200]\tSpeed: 216.1578 samples/sec\tTrain-accuracy=0.967656\n",
    "2016-08-26 20:53:43,127 - root - INFO - Epoch[10] Batch [16250]\tSpeed: 215.2490 samples/sec\tTrain-accuracy=0.970313\n",
    "2016-08-26 20:54:11,734 - root - INFO - Epoch[10] Batch [16300]\tSpeed: 223.7136 samples/sec\tTrain-accuracy=0.967812\n",
    "2016-08-26 20:54:40,546 - root - INFO - Epoch[10] Batch [16350]\tSpeed: 222.1374 samples/sec\tTrain-accuracy=0.971250\n",
    "2016-08-26 20:55:00,513 - root - INFO - Epoch[10] Resetting Data Iterator\n",
    "2016-08-26 20:55:00,513 - root - INFO - Epoch[10] Time cost=9474.580\n",
    "\n",
    "2016-08-26 21:16:02,765 - root - INFO - 0.941664950046\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
